{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb3c5ad2-cee1-4de0-86b0-06c0bd9c3552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: learning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "sentence = \"I love deep learning\"\n",
    "tokens = sentence.lower().split()\n",
    "unique_words = list(set(tokens))\n",
    "word2index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "index2word = {idx: word for word, idx in word2index.items()}\n",
    "vocab_size = len(unique_words)\n",
    "\n",
    "\n",
    "input_sequence = tokens[:3]\n",
    "target = tokens[3]\n",
    "\n",
    "def make_one_hot(index, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[index] = 1\n",
    "    return vec\n",
    "\n",
    "X = np.array([make_one_hot(word2index[word], vocab_size) for word in input_sequence])\n",
    "Y = make_one_hot(word2index[target], vocab_size)\n",
    "\n",
    "\n",
    "hidden_dim = 8\n",
    "U = np.random.randn(hidden_dim, vocab_size) * 0.01\n",
    "W = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "V = np.random.randn(vocab_size, hidden_dim) * 0.01\n",
    "b_hidden = np.zeros((hidden_dim, 1))\n",
    "b_output = np.zeros((vocab_size, 1))\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "lr = 0.1  \n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    h = np.zeros((hidden_dim, 1))\n",
    "    hidden_states = []\n",
    "\n",
    "    \n",
    "    for t in range(3):\n",
    "        xt = X[t].reshape(-1, 1)\n",
    "        h = np.tanh(np.dot(U, xt) + np.dot(W, h) + b_hidden)\n",
    "        hidden_states.append(h)\n",
    "\n",
    "    output_logits = np.dot(V, h) + b_output\n",
    "    prediction = softmax(output_logits)\n",
    "\n",
    "    loss = -np.sum(Y * np.log(prediction))  \n",
    "\n",
    "    \n",
    "    grad_output = prediction - Y.reshape(-1, 1)\n",
    "    grad_V = np.dot(grad_output, hidden_states[-1].T)\n",
    "    grad_b_output = grad_output\n",
    "\n",
    "    dh_next = np.dot(V.T, grad_output) * (1 - hidden_states[-1] ** 2)\n",
    "\n",
    "    grad_U = np.zeros_like(U)\n",
    "    grad_W = np.zeros_like(W)\n",
    "    grad_b_hidden = np.zeros_like(b_hidden)\n",
    "    h_prev = np.zeros((hidden_dim, 1))\n",
    "\n",
    "    for t in reversed(range(3)):\n",
    "        xt = X[t].reshape(-1, 1)\n",
    "        grad_U += np.dot(dh_next, xt.T)\n",
    "        grad_W += np.dot(dh_next, h_prev.T)\n",
    "        grad_b_hidden += dh_next\n",
    "        h_prev = hidden_states[t - 1] if t > 0 else np.zeros_like(h_prev)\n",
    "\n",
    "    \n",
    "    U -= lr * grad_U\n",
    "    W -= lr * grad_W\n",
    "    V -= lr * grad_V\n",
    "    b_hidden -= lr * grad_b_hidden\n",
    "    b_output -= lr * grad_b_output\n",
    "\n",
    "\n",
    "h_final = np.zeros((hidden_dim, 1))\n",
    "for t in range(3):\n",
    "    xt = X[t].reshape(-1, 1)\n",
    "    h_final = np.tanh(np.dot(U, xt) + np.dot(W, h_final) + b_hidden)\n",
    "\n",
    "logits_test = np.dot(V, h_final) + b_output\n",
    "y_out = softmax(logits_test)\n",
    "predicted_idx = np.argmax(y_out)\n",
    "\n",
    "print(\"Predicted word:\", index2word[predicted_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afe7032-c8aa-4e6a-8ae2-7f5664332b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
